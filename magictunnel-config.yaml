deployment:
  runtime_mode: advanced
server:
  host: 0.0.0.0
  port: 3001
  websocket: true
  timeout: 30
  tls:
    mode: disabled
    cert_file: null
    key_file: null
    ca_file: null
    behind_proxy: false
    trusted_proxies:
    - 10.0.0.0/8
    - 172.16.0.0/12
    - 192.168.0.0/16
    - 127.0.0.1/32
    min_tls_version: '1.2'
    cipher_suites: null
    hsts_enabled: true
    hsts_max_age: 31536000
    hsts_include_subdomains: false
    hsts_preload: false
    require_forwarded_proto: false
    require_forwarded_for: false
    auto_detect_headers:
    - X-Forwarded-Proto
    - X-Forwarded-For
    - X-Real-IP
    fallback_mode: application
registry:
  type: file
  paths:
  - /Users/gouravd/Development/magicbeanbs100x/magictunnel/capabilities
  hot_reload: true
  validation:
    strict: true
    allow_unknown_fields: false
auth: null
multi_level_auth: null
logging:
  level: info
  format: text
  file: null
external_mcp:
  enabled: true
  config_file: /Users/gouravd/Development/magicbeanbs100x/magictunnel/external-mcp-servers.yaml
  capabilities_output_dir: /Users/gouravd/Development/magicbeanbs100x/magictunnel/capabilities/external-mcp
  refresh_interval_minutes: 60
  containers:
    runtime: docker
    node_image: node:18-alpine
    python_image: python:3.11-alpine
    network_mode: bridge
    run_args:
    - --rm
    - -i
mcp_client:
  connect_timeout_secs: 30
  request_timeout_secs: 60
  max_reconnect_attempts: 5
  reconnect_delay_secs: 5
  auto_reconnect: true
  protocol_version: 2025-06-18
  client_name: magictunnel
  client_version: 0.3.5
conflict_resolution:
  strategy: LocalFirst
  local_prefix: local
  proxy_prefix_format: '{server}'
  log_conflicts: true
  include_conflict_metadata: true
visibility:
  hide_individual_tools: true
  expose_smart_discovery_only: true
  allow_override: true
  default_hidden: true
smart_discovery:
  enabled: true
  tool_selection_mode: hybrid
  default_confidence_threshold: 0.7
  max_tools_to_consider: 10
  max_high_quality_matches: 3
  high_quality_threshold: 0.95
  use_fuzzy_matching: true
  llm_mapper:
    provider: openai
    model: gpt-4o-mini
    api_key: null
    api_key_env: OPENAI_API_KEY
    base_url: null
    timeout: 30
    max_retries: 3
    enabled: true
  llm_tool_selection:
    enabled: true
    provider: openai
    model: gpt-4o-mini
    api_key: null
    api_key_env: OPENAI_API_KEY
    base_url: null
    timeout: 30
    max_retries: 3
    batch_size: 15
    max_context_tokens: 4000
  cache:
    max_tool_matches: 1000
    tool_match_ttl: 3600
    max_llm_responses: 500
    llm_response_ttl: 1800
    max_registry_entries: 100
    registry_ttl: 300
    enabled: true
  fallback:
    enabled: true
    min_confidence_threshold: 0.3
    max_fallback_suggestions: 5
    enable_fuzzy_fallback: true
    enable_keyword_fallback: true
    enable_category_fallback: true
    enable_partial_match_fallback: true
  semantic_search:
    enabled: true
    model_name: ollama:nomic-embed-text
    similarity_threshold: 0.55
    max_results: 10
    storage:
      embeddings_file: /Users/gouravd/Development/magicbeanbs100x/magictunnel/data/embeddings/tool_embeddings.bin
      metadata_file: /Users/gouravd/Development/magicbeanbs100x/magictunnel/data/embeddings/tool_metadata.json
      hash_file: /Users/gouravd/Development/magicbeanbs100x/magictunnel/data/embeddings/content_hashes.json
      backup_count: 3
      auto_backup: true
      compression: true
    model:
      cache_dir: /Users/gouravd/Development/magicbeanbs100x/magictunnel/data/models
      device: cpu
      max_sequence_length: 512
      batch_size: 32
      normalize_embeddings: true
    performance:
      lazy_loading: true
      embedding_cache_size: 1000
      parallel_processing: true
      worker_threads: 4
  enable_sequential_mode: true
  tool_metrics_enabled: null
security:
  enabled: true
  allowlist:
    enabled: true
    default_action: deny
    emergency_lockdown: false
    tools: {}
    servers: {}
    capability_patterns: []
    global_patterns: []
  sanitization:
    enabled: false
    policies: []
    default_action:
      type: LogAndAllow
      level: warn
    log_actions: true
  rbac:
    enabled: false
    roles: {}
    user_roles: {}
    api_key_roles: {}
    default_roles: []
    inherit_permissions: true
  audit:
    enabled: true
    events:
    - authentication
    - authorization
    - tool_execution
    - security_violation
    storage:
      type: File
      directory: ./logs
      rotation:
        max_file_size: 104857600
        max_files: 10
        compress: true
    retention_days: 90
    include_bodies: false
    max_body_size: 1024
    mask_sensitive_data: true
  emergency_lockdown:
    enabled: true
    state_file_path: ./data/emergency_lockdown.json
    log_blocked_requests: true
    authorized_users: []
streamable_http: null
sampling:
  enabled: true
  default_sampling_strategy: client_forwarded
  default_elicitation_strategy: null
  llm_config:
    provider: openai
    model: gpt-4o-mini
    api_key_env: OPENAI_API_KEY
    api_base_url: null
    max_tokens: 4000
    temperature: null
    additional_params: null
tool_enhancement:
  enabled: true
  llm_config:
    provider: openai
    model: gpt-4o-mini
    api_key_env: OPENAI_API_KEY
    api_base_url: null
    max_tokens: 4000
    temperature: 0.7
    additional_params: null
elicitation:
  enabled: true
  default_elicitation_strategy: client_forwarded
prompt_generation: null
resource_generation: null
content_storage: null
external_content: null
enhancement_storage:
  storage_dir: ./storage/enhanced_tools
  max_storage_mb: 512
  cleanup_policy:
    max_age_days: 90
    max_versions_per_tool: 5
    cleanup_on_startup: true
  enable_versioning: true
  auto_load_on_startup: true
