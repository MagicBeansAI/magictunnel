# MagicTunnel Configuration Template
# Copy this file to config.yaml and customize for your environment
# All settings can be overridden via environment variables
#
# IMPORTANT: Legacy mcp_proxy, mcp_servers, and remote_mcp configurations have been removed.
# Use the new external_mcp system for connecting to external MCP servers.

# =============================================================================
# SERVER CONFIGURATION
# =============================================================================
server:
  host: "0.0.0.0"        # Server bind address (env: MCP_HOST)
  port: 3001              # Server port 1-65535 (env: MCP_PORT)
                           # Note: gRPC server automatically runs on port + 1000 (e.g., 4000)
  websocket: true          # Enable WebSocket support (env: MCP_WEBSOCKET)
  timeout: 30              # Request timeout in seconds (env: MCP_TIMEOUT)

  # TLS/SSL Configuration (Optional)
  tls:
    mode: "disabled"       # TLS mode: disabled|application|behind_proxy|auto
    cert_file: null        # Path to certificate file (PEM format)
    key_file: null         # Path to private key file (PEM format)
    ca_file: null          # Path to CA certificate file (optional)
    behind_proxy: false    # Whether running behind a reverse proxy
    trusted_proxies:       # List of trusted proxy IP ranges (CIDR notation)
      - "10.0.0.0/8"
      - "172.16.0.0/12"
      - "192.168.0.0/16"
      - "127.0.0.1/32"
    min_tls_version: "1.2" # Minimum TLS version (1.2, 1.3)
    cipher_suites: null    # Custom cipher suites (optional)
    hsts_enabled: true     # Enable HTTP Strict Transport Security (HSTS)
    hsts_max_age: 31536000 # HSTS max age in seconds (1 year)
    hsts_include_subdomains: false
    hsts_preload: false
    require_forwarded_proto: false
    require_forwarded_for: false
    auto_detect_headers:   # Auto-detection headers to check
      - "X-Forwarded-Proto"
      - "X-Forwarded-For"
      - "X-Real-IP"
    fallback_mode: "application"

# =============================================================================
# CAPABILITY REGISTRY CONFIGURATION
# =============================================================================
registry:
  type: "file"             # Registry type: "file" (env: MCP_REGISTRY_TYPE)
  paths:                   # Paths to scan for capability files (env: MCP_REGISTRY_PATHS)
    - "./capabilities"     # Default capabilities directory
                           # Note: External capabilities are auto-generated by external_mcp discovery
  hot_reload: true         # Enable file watching for changes (env: MCP_HOT_RELOAD)
  validation:
    strict: true           # Strict validation mode
    allow_unknown_fields: false



# =============================================================================
# AUTHENTICATION CONFIGURATION (Optional)
# =============================================================================
# Uncomment and configure one of the following authentication methods:

# API Key Authentication
# auth:
#   enabled: true
#   type: "api_key"
#   api_keys:
#     keys:
#       - key: "your_secure_api_key_here_min_16_chars"
#         name: "Admin Key"
#         description: "Administrative access"
#         permissions: ["read", "write", "admin"]
#         active: true
#         expires_at: "2025-12-31T23:59:59Z"  # Optional ISO 8601 format
#       - key: "readonly_key_here_min_16_chars"
#         name: "Read Only Key"
#         description: "Read-only access"
#         permissions: ["read"]
#         active: true
#     require_header: true
#     header_name: "Authorization"
#     header_format: "Bearer {key}"

# OAuth Authentication
# auth:
#   enabled: true
#   type: "oauth"
#   oauth:
#     provider: "google"    # or "github", "microsoft", "custom"
#     client_id: "your-oauth-client-id"
#     client_secret: "your-oauth-client-secret"
#     auth_url: "https://accounts.google.com/oauth/authorize"
#     token_url: "https://oauth2.googleapis.com/token"

# JWT Authentication
# auth:
#   enabled: true
#   type: "jwt"
#   jwt:
#     secret: "your_jwt_secret_key_at_least_32_characters_long"
#     algorithm: "HS256"     # HS256, HS384, HS512, RS256, RS384, RS512, ES256, ES384
#     expiration: 3600       # Token expiration in seconds
#     issuer: "magictunnel"  # Optional JWT issuer
#     audience: "mcp-clients" # Optional JWT audience

# =============================================================================
# MCP CLIENT CONFIGURATION
# =============================================================================
# Configuration for MCP client connections (used when connecting to external MCP servers)
mcp_client:
  connect_timeout_secs: 30      # Connection timeout in seconds
  request_timeout_secs: 60      # Request timeout in seconds
  max_reconnect_attempts: 5     # Maximum reconnection attempts
  reconnect_delay_secs: 5       # Delay between reconnection attempts
  auto_reconnect: true          # Enable automatic reconnection
  protocol_version: "2025-06-18" # MCP protocol version to use
  client_name: "magictunnel"    # Client name for MCP handshake (defaults to package name)
  client_version: "0.3.0"       # Client version for MCP handshake (defaults to package version)

# =============================================================================
# EXTERNAL MCP CONFIGURATION (Claude Desktop Format)
# =============================================================================
# Enable discovery and integration of external MCP servers using Claude Desktop's
# exact configuration format for maximum compatibility.
#
# MIGRATION NOTE: The legacy mcp_proxy, mcp_servers, and remote_mcp configurations
# have been removed. Use this external_mcp system instead.
external_mcp:
  enabled: true                     # Enable external MCP discovery (env: EXTERNAL_MCP_ENABLED)
  config_file: "/Users/gouravd/Development/magicbeanbs100x/magictunnel/external-mcp-servers.yaml"  # Path to external MCP servers config file
  capabilities_output_dir: "/Users/gouravd/Development/magicbeanbs100x/magictunnel/capabilities/external-mcp"  # Where to generate capability files
  refresh_interval_minutes: 60      # How often to refresh capabilities (env: EXTERNAL_MCP_REFRESH_INTERVAL)

  # Container Configuration (for Docker/Podman MCP servers)
  containers:
    runtime: "docker"               # Container runtime: docker|podman (env: CONTAINER_RUNTIME)
    node_image: "node:18-alpine"    # Default Node.js image for MCP servers
    python_image: "python:3.11-alpine"  # Default Python image for MCP servers
    network_mode: "bridge"          # Container network mode
    run_args: ["--rm", "-i"]        # Additional container run arguments

  # MCP External Routing Configuration (MCP 2025-06-18)
  # Configuration for routing to external MCP servers for sampling and elicitation requests
  external_routing:
    enabled: true                   # Enable MCP external routing for sampling/elicitation (env: EXTERNAL_MCP_ROUTING_ENABLED)
    
    # Sampling Request Routing Strategy
    sampling:
      strategy: "priority_with_fallback"  # Strategy: priority_with_fallback|round_robin|first_available (env: EXTERNAL_MCP_SAMPLING_STRATEGY)
      priority_order:               # Priority order for sampling-capable servers (highest to lowest priority)
        - "claude-mcp-server"       # Example: Premium external MCP server with advanced models
        - "openai-mcp-server"       # Example: OpenAI-based MCP server
        - "ollama-local"            # Example: Local Ollama MCP server
      fallback_to_internal: true   # Fall back to internal sampling service if all external servers fail (env: EXTERNAL_MCP_SAMPLING_FALLBACK)
      max_retry_attempts: 2         # Maximum retry attempts per server before trying next (env: EXTERNAL_MCP_SAMPLING_MAX_RETRIES)
      timeout_seconds: 30           # Timeout for each sampling request (env: EXTERNAL_MCP_SAMPLING_TIMEOUT)
    
    # Elicitation Request Routing Strategy  
    elicitation:
      strategy: "first_available"   # Strategy for elicitation requests (env: EXTERNAL_MCP_ELICITATION_STRATEGY)
      priority_order: []            # Custom priority order (empty = use discovery order)
      fallback_to_internal: true   # Fall back to internal elicitation service (env: EXTERNAL_MCP_ELICITATION_FALLBACK)
      max_retry_attempts: 1         # Elicitation typically needs fewer retries (env: EXTERNAL_MCP_ELICITATION_MAX_RETRIES)
      timeout_seconds: 60           # Longer timeout for elicitation (may involve user interaction) (env: EXTERNAL_MCP_ELICITATION_TIMEOUT)

  # Note: External MCP uses the global conflict_resolution configuration below

# =============================================================================
# GLOBAL CONFLICT RESOLUTION CONFIGURATION
# =============================================================================
# Configuration for resolving conflicts when tools from different sources have the same name
# This applies to conflicts between local tools and external MCP tools

conflict_resolution:
  # Strategy for resolving tool name conflicts
  # Options: local_first, proxy_first, first_found, reject, prefix
  strategy: "LocalFirst"         # Strategy: LocalFirst|ProxyFirst|FirstFound|Reject|Prefix (env: CONFLICT_RESOLUTION_STRATEGY)

  # Prefix for local tools when using prefix strategy
  local_prefix: "local"           # Prefix for local tools (env: CONFLICT_RESOLUTION_LOCAL_PREFIX)

  # Format for external/proxy tool prefixes when using prefix strategy
  # Use {server} placeholder for server name
  proxy_prefix_format: "{server}" # Format for external tool prefixes (env: CONFLICT_RESOLUTION_PROXY_PREFIX_FORMAT)

  # Whether to log conflict resolutions
  log_conflicts: true             # Log conflicts (env: CONFLICT_RESOLUTION_LOG_CONFLICTS)

  # Whether to include conflict metadata in tool definitions
  include_conflict_metadata: true # Include metadata (env: CONFLICT_RESOLUTION_INCLUDE_METADATA)

# =============================================================================
# SMART DISCOVERY CONFIGURATION
# =============================================================================
# Configuration for the Smart Tool Discovery system that provides intelligent
# tool selection based on natural language requests

smart_discovery:
  enabled: true                           # Enable smart discovery (env: SMART_DISCOVERY_ENABLED)
  tool_selection_mode: "hybrid"          # Tool selection mode: rule_based|llm_based|semantic_based|hybrid (env: SMART_DISCOVERY_MODE)
  default_confidence_threshold: 0.7       # Default confidence threshold for tool matching (env: SMART_DISCOVERY_THRESHOLD)
  
  # MCP 2025-06-18 Enhanced Features
  enable_sampling: true                   # Enable sampling service for enhanced LLM interactions (env: SMART_DISCOVERY_SAMPLING_ENABLED)
  enable_elicitation: true                # Enable elicitation service for structured data collection (env: SMART_DISCOVERY_ELICITATION_ENABLED)
  max_tools_to_consider: 10               # Maximum number of tools to consider for matching (env: SMART_DISCOVERY_MAX_TOOLS)
  max_high_quality_matches: 3             # Maximum high-quality matches to collect before stopping processing (env: SMART_DISCOVERY_MAX_HIGH_QUALITY_MATCHES)
  high_quality_threshold: 0.95            # Confidence threshold for considering a match as high-quality (env: SMART_DISCOVERY_HIGH_QUALITY_THRESHOLD)
  use_fuzzy_matching: true                # Enable fuzzy matching for tool names (env: SMART_DISCOVERY_FUZZY)
  enable_sequential_mode: true            # Enable sequential mode for multi-step workflows (default: true) (env: SMART_DISCOVERY_SEQUENTIAL_MODE)

  # LLM Tool Selection Configuration (for llm_based mode)
  llm_tool_selection:
    enabled: true                         # Enable LLM-based tool selection (env: SMART_DISCOVERY_LLM_ENABLED)
    provider: "openai"                    # LLM provider: openai|anthropic|ollama (env: SMART_DISCOVERY_LLM_PROVIDER)
    model: "gpt-4o-mini"                  # Model name to use (env: SMART_DISCOVERY_LLM_MODEL)
    api_key: null                         # API key (set via api_key_env)
    api_key_env: "OPENAI_API_KEY"         # Environment variable for API key (env: SMART_DISCOVERY_LLM_API_KEY_ENV)
    base_url: null                        # Custom base URL (env: SMART_DISCOVERY_LLM_BASE_URL)
    timeout: 30                           # Request timeout in seconds (env: SMART_DISCOVERY_LLM_TIMEOUT)
    max_retries: 3                        # Maximum retries for failed requests (env: SMART_DISCOVERY_LLM_MAX_RETRIES)
    batch_size: 15                        # Batch size for processing tools (env: SMART_DISCOVERY_LLM_BATCH_SIZE)
    max_context_tokens: 4000              # Maximum context tokens to use (env: SMART_DISCOVERY_LLM_MAX_TOKENS)

  # LLM Parameter Mapping Configuration
  llm_mapper:
    enabled: true                         # Enable LLM parameter mapping (env: SMART_DISCOVERY_MAPPER_ENABLED)
    provider: "openai"                    # LLM provider: openai|anthropic|ollama (env: SMART_DISCOVERY_MAPPER_PROVIDER)
    model: "gpt-4o-mini"                  # Model name to use (env: SMART_DISCOVERY_MAPPER_MODEL)
    api_key_env: "OPENAI_API_KEY"         # Environment variable for API key (env: SMART_DISCOVERY_MAPPER_API_KEY_ENV)
    base_url: null                        # Custom base URL (env: SMART_DISCOVERY_MAPPER_BASE_URL)
    timeout: 30                           # Request timeout in seconds (env: SMART_DISCOVERY_MAPPER_TIMEOUT)
    max_retries: 3                        # Maximum retries for failed requests (env: SMART_DISCOVERY_MAPPER_MAX_RETRIES)

  # Cache Configuration
  cache:
    enabled: true                         # Enable caching (env: SMART_DISCOVERY_CACHE_ENABLED)
    max_tool_matches: 1000               # Maximum number of entries in tool matching cache
    tool_match_ttl: 3600                 # TTL for tool matching cache entries (seconds)
    max_llm_responses: 500               # Maximum number of entries in LLM response cache
    llm_response_ttl: 1800               # TTL for LLM response cache entries (seconds)
    max_registry_entries: 100            # Maximum number of entries in registry cache
    registry_ttl: 300                    # TTL for registry cache entries (seconds)

  # Fallback Configuration
  fallback:
    enabled: true                         # Enable fallback suggestions (env: SMART_DISCOVERY_FALLBACK_ENABLED)
    min_confidence_threshold: 0.3         # Minimum confidence for fallback suggestions (env: SMART_DISCOVERY_FALLBACK_MIN_CONFIDENCE)
    max_fallback_suggestions: 5           # Maximum number of fallback suggestions (env: SMART_DISCOVERY_FALLBACK_MAX_SUGGESTIONS)
    enable_fuzzy_fallback: true           # Enable fuzzy matching fallback (env: SMART_DISCOVERY_FALLBACK_FUZZY)
    enable_keyword_fallback: true         # Enable keyword-based fallback (env: SMART_DISCOVERY_FALLBACK_KEYWORDS)
    enable_category_fallback: true        # Enable category-based fallback (env: SMART_DISCOVERY_FALLBACK_CATEGORIES)
    enable_partial_match_fallback: true   # Enable partial match fallback (env: SMART_DISCOVERY_FALLBACK_PARTIAL)

  # Semantic Search Configuration
  semantic_search:
    enabled: true                         # Enable semantic search (env: SMART_DISCOVERY_SEMANTIC_ENABLED)
    model_name: "ollama:nomic-embed-text"   # Embedding model name - Options:
                                          # 
                                          # RECOMMENDED OPTIONS (fully working):
                                          # - "ollama:nomic-embed-text" (768-dim, Ollama server) - BEST FOR LOCAL DEVELOPMENT
                                          # - "openai:text-embedding-3-small" (1536-dim, OpenAI API) - BEST FOR PRODUCTION
                                          # - "external:api" (custom embedding API via EMBEDDING_API_URL) - CUSTOM SETUPS
                                          #
                                          # CLOUD MODELS (API key required):
                                          # - "openai:text-embedding-3-large" (3072-dim, OpenAI API) - PREMIUM QUALITY
                                          #
                                          # FALLBACK OPTIONS (deterministic hashing - not recommended for production):
                                          # - "all-MiniLM-L6-v2" (384-dim) - Uses hash-based fallback, not real embeddings
                                          # - "all-mpnet-base-v2" (768-dim) - Uses hash-based fallback, not real embeddings
                                          # - "local:/path/to/model" (custom model) - Uses hash-based fallback
                                          # 
                                          # SETUP INSTRUCTIONS:
                                          # For Ollama: ollama pull nomic-embed-text && export OLLAMA_BASE_URL=http://localhost:11434
                                          # For OpenAI: export OPENAI_API_KEY=your-key-here
                                          # For External: export EMBEDDING_API_URL=http://your-server:8080
                                          # 
                                          # Environment override: MAGICTUNNEL_SEMANTIC_MODEL
    similarity_threshold: 0.55             # Minimum similarity threshold for semantic matches (env: SMART_DISCOVERY_SEMANTIC_THRESHOLD)
    max_results: 10                       # Maximum number of semantic search results (env: SMART_DISCOVERY_SEMANTIC_MAX_RESULTS)
    
    # Persistent Storage Configuration
    storage:
      embeddings_file: "/Users/gouravd/Development/magicbeanbs100x/magictunnel/data/embeddings/tool_embeddings.bin"  # Binary file for embeddings storage
      metadata_file: "/Users/gouravd/Development/magicbeanbs100x/magictunnel/data/embeddings/tool_metadata.json"     # JSON file for tool metadata
      hash_file: "/Users/gouravd/Development/magicbeanbs100x/magictunnel/data/embeddings/content_hashes.json"        # JSON file for content hash validation
      backup_count: 3                     # Number of backup files to maintain
      auto_backup: true                   # Automatically backup embeddings on updates
      compression: true                   # Enable compression for storage files
    
    # Model Configuration
    model:
      cache_dir: "/Users/gouravd/Development/magicbeanbs100x/magictunnel/data/models"          # Directory to cache downloaded models (env: SMART_DISCOVERY_SEMANTIC_CACHE_DIR)
      device: "cpu"                       # Device to use: cpu|cuda|mps (env: SMART_DISCOVERY_SEMANTIC_DEVICE)
      max_sequence_length: 512            # Maximum sequence length for embeddings
      batch_size: 32                      # Batch size for embedding generation
      normalize_embeddings: true          # Normalize embeddings to unit vectors
      
    # Performance Configuration
    performance:
      lazy_loading: true                  # Load embeddings only when needed
      embedding_cache_size: 1000          # In-memory cache size for embeddings
      parallel_processing: true           # Enable parallel embedding generation
      worker_threads: 4                   # Number of worker threads for parallel processing

# =============================================================================
# MCP 2025-06-18 SAMPLING SERVICE CONFIGURATION
# =============================================================================
# Configuration for the Sampling service that enables structured LLM message generation
sampling:
  enabled: true                           # Enable sampling service (auto-enabled if smart_discovery.enable_sampling is true)
  
  # Default LLM Configuration (inherits from smart_discovery.llm_mapper if not specified)
  default_model: "gpt-4o-mini"           # Default model for sampling requests
  max_tokens_limit: 4000                 # Maximum tokens allowed per request
  
  # Rate Limiting
  rate_limit:
    requests_per_minute: 60              # Requests per minute per user
    burst_size: 10                       # Burst size for rate limiting
    window_seconds: 60                   # Rate limiting window in seconds
  
  # Content Security
  content_filter:
    enabled: true                        # Enable content filtering
    max_content_length: 50000           # Maximum content length
    blocked_patterns:                    # Content patterns to block (regex)
      - "(?i)(password|secret|key|token|credential)"
      - "(?i)(hack|exploit|vulnerability|injection)"
    approval_patterns: []                # Patterns requiring approval (empty = none)
  
  # LLM Provider Configuration (uses smart_discovery providers by default)
  providers:
    - name: "openai"
      type: "openai"
      endpoint: "https://api.openai.com/v1"
      models: ["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo"]
    - name: "ollama"
      type: "ollama" 
      endpoint: "http://localhost:11434"
      models: ["llama3.2", "qwen2.5"]

# =============================================================================
# MCP 2025-06-18 ELICITATION SERVICE CONFIGURATION
# =============================================================================
# Configuration for the Elicitation service that enables structured data collection
elicitation:
  enabled: true                          # Enable elicitation service (auto-enabled if smart_discovery.enable_elicitation is true)
  
  # Schema Configuration
  max_schema_complexity: "WithArrays"    # Maximum schema complexity: Simple|WithArrays|Nested|Complex
  default_timeout_seconds: 300           # Default timeout for elicitation requests (5 minutes)
  max_timeout_seconds: 1800             # Maximum timeout allowed (30 minutes)
  
  # Rate Limiting
  rate_limit:
    requests_per_minute: 10              # Lower limit for elicitation (more resource intensive)
    burst_size: 3                        # Smaller burst size
    window_seconds: 60                   # Rate limiting window in seconds
  
  # Security Configuration
  security:
    enabled: true                        # Enable security checks
    max_fields: 20                       # Maximum number of fields in schema
    min_privacy_level: "Internal"        # Minimum privacy level: Public|Internal|Confidential|Restricted
    log_requests: true                   # Log all elicitation requests for audit
    blocked_field_names:                 # Field names that are not allowed
      - "password"
      - "secret"
      - "private_key"
      - "api_key"
      - "auth_token"
      - "ssn"
      - "social_security_number"
      - "credit_card"
      - "bank_account"
    blocked_schema_patterns:             # Schema patterns to block (regex)
      - "(?i)(password|secret|key|token|credential)"
      - "(?i)(ssn|social.*security|credit.*card|bank.*account)"
      - "(?i)(private.*key|api.*key|auth.*token)"
  
  # Schema Validation Configuration
  schema_validation:
    strict_validation: true              # Enable strict schema validation
    max_string_length: 1000             # Maximum string field length
    max_number_value: 1000000000000     # Maximum numeric value (1e12)
    min_number_value: -1000000000000    # Minimum numeric value (-1e12)
    allowed_types:                       # JSON schema types allowed
      - "string"
      - "number" 
      - "integer"
      - "boolean"
      - "array"


# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
logging:
  level: "info"            # Log level: debug|info|notice|warning|error|critical|alert|emergency (env: MCP_LOG_LEVEL)
  format: "text"           # Log format: json|text (env: MCP_LOG_FORMAT)
  file: null               # Optional: log to file instead of stdout
