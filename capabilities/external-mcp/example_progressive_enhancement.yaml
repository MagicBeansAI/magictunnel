# Progressive Enhancement Example - Non-Breaking Changes
# Existing tools work unchanged, new features are opt-in

metadata:
  name: "Enhanced File Operations"
  description: "File operations with MCP 2025-06-18 enhancements"
  version: "2.0.0"
  author: "MCP Team"
  tags: ["filesystem", "enhanced", "mcp-2025"]
  
  # NEW: MCP 2025-06-18 metadata
  mcp_2025_features:
    enhanced_discovery: true
    security_validation: true
    progress_tracking: true
    cancellation_support: true

tools:
  # Standard tool - works exactly as before
  - name: "read_file"
    description: "Read contents of a file"
    inputSchema:
      type: object
      properties:
        path:
          type: string
          description: "File path to read"
      required: ["path"]
    routing:
      type: subprocess
      config:
        command: cat
        args: ["{path}"]
        timeout: 30
    hidden: true
    
    # NEW: Optional MCP 2025-06-18 enhancements
    mcp_2025:
      security:
        classification: "safe"           # safe, restricted, privileged, dangerous, blocked
        sandbox_policy: "filesystem_read"
        requires_approval: false
        
      performance:
        estimated_duration_seconds: 5
        complexity_level: "simple"      # simple, moderate, complex, very_complex
        supports_cancellation: true
        supports_progress_tracking: false
        
      discovery:
        llm_enhanced_description: >
          Reads and returns the complete contents of any text file. 
          Perfect for configuration files, logs, documentation, code files, and data files.
          Supports various encodings and handles large files efficiently.
        usage_examples:
          - "read the config.yaml file"
          - "show me the contents of README.md"
          - "read the error log file"
        semantic_tags: ["read", "file", "contents", "text", "data"]
        parameter_suggestions:
          path:
            common_values: ["config.yaml", "README.md", "/var/log/error.log"]
            validation_rules: ["must_exist", "readable"]

  # Enhanced tool taking full advantage of new features
  - name: "process_large_dataset"
    description: "Process large datasets with progress tracking and cancellation"
    inputSchema:
      type: object
      properties:
        input_file:
          type: string
          description: "Path to input dataset"
        output_file:
          type: string
          description: "Path for processed output"
        processing_type:
          type: string
          enum: ["csv_analysis", "json_transform", "data_cleanup"]
        batch_size:
          type: integer
          default: 1000
          minimum: 100
          maximum: 10000
      required: ["input_file", "output_file", "processing_type"]
    routing:
      type: subprocess
      config:
        command: "python3"
        args: ["process_dataset.py", "{input_file}", "{output_file}", "{processing_type}"]
        timeout: 3600
    hidden: true
    
    # Full MCP 2025-06-18 integration
    mcp_2025:
      security:
        classification: "restricted"
        sandbox_policy: "data_processing"
        requires_approval: false
        allowed_paths: ["/data/input/*", "/data/output/*"]
        blocked_patterns: ["/etc/*", "/root/*"]
        
      performance:
        estimated_duration_seconds: 300
        complexity_level: "complex"
        supports_cancellation: true
        supports_progress_tracking: true
        progress_granularity: "detailed"    # basic, standard, detailed, verbose
        
      discovery:
        llm_enhanced_description: >
          Advanced dataset processing tool that handles large CSV, JSON, and structured data files.
          Provides real-time progress updates, supports cancellation, and processes data in configurable batches.
          Ideal for data cleaning, transformation, analysis, and format conversion tasks.
        usage_examples:
          - "process the sales data CSV file and clean it up"
          - "transform the JSON export to CSV format"
          - "analyze the customer dataset for duplicates"
        semantic_tags: ["data", "process", "csv", "json", "analysis", "transform", "large", "batch"]
        workflow_patterns:
          - follows: ["read_file", "list_directory"]
          - precedes: ["generate_report", "send_email"]
        parameter_suggestions:
          processing_type:
            recommendations:
              - value: "csv_analysis"
                when: "input_file contains '.csv'"
              - value: "json_transform" 
                when: "input_file contains '.json'"
          batch_size:
            recommendations:
              - value: 500
                when: "file_size < 10MB"
              - value: 2000
                when: "file_size > 100MB"