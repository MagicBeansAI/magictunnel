# Performance Testing and Benchmarking Showcase
# This demonstrates performance testing capabilities and benchmarking
# for different agent types and routing scenarios

metadata:
  name: "Performance Testing Showcase"
  description: "Performance testing and benchmarking for agent routing system"
  version: "1.0.0"
  author: "MCP Proxy Team"
  tags: ["testing", "performance", "benchmarking", "load-testing", "metrics"]

tools:
  # ===== SUBPROCESS PERFORMANCE TESTS =====
  
  - name: "subprocess_benchmark"
    description: "Benchmark subprocess execution performance with various workloads"
    inputSchema:
      type: "object"
      properties:
        workload_type:
          type: "string"
          description: "Type of workload to benchmark"
          enum: ["cpu_intensive", "io_intensive", "memory_intensive", "quick_tasks"]
          default: "quick_tasks"
        iterations:
          type: "integer"
          description: "Number of iterations to run"
          default: 10
          minimum: 1
          maximum: 100
        complexity:
          type: "string"
          description: "Complexity level"
          enum: ["low", "medium", "high"]
          default: "medium"
      required: []
    routing:
      type: "subprocess"
      config:
        command: "bash"
        args: [
          "-c",
          "{{#switch workload_type}}{{#case 'cpu_intensive'}}for i in $(seq 1 {{iterations}}); do echo 'scale=1000; 4*a(1)' | bc -l > /dev/null; done{{/case}}{{#case 'io_intensive'}}for i in $(seq 1 {{iterations}}); do dd if=/dev/zero of=/tmp/test_$i bs=1M count={{complexity === 'high' ? 10 : complexity === 'medium' ? 5 : 1}} 2>/dev/null; rm /tmp/test_$i; done{{/case}}{{#case 'memory_intensive'}}python3 -c 'import time; [bytearray({{complexity === 'high' ? 10 : complexity === 'medium' ? 5 : 1}} * 1024 * 1024) for _ in range({{iterations}})]'{{/case}}{{#case 'quick_tasks'}}for i in $(seq 1 {{iterations}}); do echo 'Task $i completed'; done{{/case}}{{/switch}}"
        ]
        timeout: 300
    hidden: true

  - name: "parallel_subprocess_test"
    description: "Test parallel subprocess execution performance"
    inputSchema:
      type: "object"
      properties:
        parallel_count:
          type: "integer"
          description: "Number of parallel processes"
          default: 4
          minimum: 1
          maximum: 16
        task_duration:
          type: "integer"
          description: "Duration of each task in seconds"
          default: 2
          minimum: 1
          maximum: 30
        resource_type:
          type: "string"
          description: "Type of resource to test"
          enum: ["cpu", "io", "mixed"]
          default: "mixed"
      required: []
    routing:
      type: "subprocess"
      config:
        command: "bash"
        args: [
          "-c",
          "echo 'Starting parallel test with {{parallel_count}} processes'; for i in $(seq 1 {{parallel_count}}); do ({{#switch resource_type}}{{#case 'cpu'}}yes > /dev/null{{/case}}{{#case 'io'}}find / -name '*.txt' 2>/dev/null{{/case}}{{#case 'mixed'}}dd if=/dev/zero of=/dev/null bs=1M count=100 2>/dev/null{{/case}}{{/switch}} & sleep {{task_duration}}; kill $!) & done; wait; echo 'All processes completed'"
        ]
        timeout: 120
    hidden: true

  # ===== HTTP PERFORMANCE TESTS =====

  - name: "http_latency_test"
    description: "Measure HTTP request latency and throughput"
    inputSchema:
      type: "object"
      properties:
        target_url:
          type: "string"
          description: "URL to test latency against"
          default: "https://httpbin.org/get"
          format: "uri"
        request_count:
          type: "integer"
          description: "Number of requests to send"
          default: 10
          minimum: 1
          maximum: 100
        concurrent_requests:
          type: "integer"
          description: "Number of concurrent requests"
          default: 1
          minimum: 1
          maximum: 10
        payload_size:
          type: "string"
          description: "Size of request payload"
          enum: ["small", "medium", "large"]
          default: "small"
      required: []
    routing:
      type: "http"
      config:
        method: "{{payload_size === 'small' ? 'GET' : 'POST'}}"
        url: "{{target_url}}"
        headers:
          X-Performance-Test: "latency-measurement"
          X-Request-Count: "{{request_count}}"
          X-Concurrent: "{{concurrent_requests}}"
          X-Payload-Size: "{{payload_size}}"
        body: "{{#if (payload_size !== 'small')}}{{#switch payload_size}}{{#case 'medium'}}{{'x'.repeat(1024)}}{{/case}}{{#case 'large'}}{{'x'.repeat(10240)}}{{/case}}{{/switch}}{{/if}}"
        timeout: 60
    hidden: true

  - name: "api_stress_test"
    description: "Stress test API endpoints with high load"
    inputSchema:
      type: "object"
      properties:
        endpoint_url:
          type: "string"
          description: "API endpoint to stress test"
          default: "https://httpbin.org/anything"
          format: "uri"
        ramp_up_time:
          type: "integer"
          description: "Ramp up time in seconds"
          default: 10
          minimum: 1
          maximum: 60
        peak_rps:
          type: "integer"
          description: "Peak requests per second"
          default: 10
          minimum: 1
          maximum: 100
        test_duration:
          type: "integer"
          description: "Test duration in seconds"
          default: 30
          minimum: 10
          maximum: 300
      required: []
    routing:
      type: "http"
      config:
        method: "POST"
        url: "{{endpoint_url}}"
        headers:
          Content-Type: "application/json"
          X-Stress-Test: "true"
          X-Peak-RPS: "{{peak_rps}}"
          X-Test-Duration: "{{test_duration}}"
        body: |
          {
            "test_type": "stress_test",
            "timestamp": "{{now}}",
            "ramp_up_time": {{ramp_up_time}},
            "peak_rps": {{peak_rps}},
            "test_duration": {{test_duration}},
            "payload": "{{uuid}}"
          }
        timeout: 30
    hidden: true

  # ===== LLM PERFORMANCE TESTS =====

  - name: "llm_response_time_test"
    description: "Measure LLM response times for different prompt types"
    inputSchema:
      type: "object"
      properties:
        prompt_type:
          type: "string"
          description: "Type of prompt to test"
          enum: ["short_question", "code_generation", "long_analysis", "creative_writing"]
          default: "short_question"
        model_type:
          type: "string"
          description: "Model to test"
          enum: ["gpt-4", "gpt-3.5-turbo", "claude-3-sonnet"]
          default: "gpt-3.5-turbo"
        token_limit:
          type: "integer"
          description: "Maximum tokens to generate"
          default: 500
          minimum: 50
          maximum: 2000
        temperature:
          type: "number"
          description: "Temperature setting"
          default: 0.7
          minimum: 0.0
          maximum: 2.0
      required: []
    routing:
      type: "llm"
      config:
        provider: "{{model_type.startsWith('gpt') ? 'openai' : 'anthropic'}}"
        model: "{{model_type}}"
        api_key: "{{model_type.startsWith('gpt') ? env.OPENAI_API_KEY : env.ANTHROPIC_API_KEY}}"
        timeout: 120
        max_tokens: "{{token_limit}}"
        temperature: "{{temperature}}"
        system_prompt: "You are a performance testing assistant. Respond efficiently and accurately."
        user_prompt: "{{#switch prompt_type}}{{#case 'short_question'}}What is the capital of France?{{/case}}{{#case 'code_generation'}}Write a Python function to calculate the factorial of a number using recursion.{{/case}}{{#case 'long_analysis'}}Analyze the pros and cons of microservices architecture versus monolithic architecture in detail, considering scalability, maintainability, deployment complexity, and team organization.{{/case}}{{#case 'creative_writing'}}Write a short story about a robot who discovers emotions for the first time.{{/case}}{{/switch}}"
    hidden: true

  - name: "llm_throughput_test"
    description: "Test LLM throughput with batch processing"
    inputSchema:
      type: "object"
      properties:
        batch_size:
          type: "integer"
          description: "Number of prompts in batch"
          default: 5
          minimum: 1
          maximum: 20
        prompt_complexity:
          type: "string"
          description: "Complexity of prompts"
          enum: ["simple", "moderate", "complex"]
          default: "moderate"
        model_preference:
          type: "string"
          description: "Model to use for testing"
          enum: ["fastest", "balanced", "highest_quality"]
          default: "balanced"
      required: []
    routing:
      type: "llm"
      config:
        provider: "openai"
        model: "{{#switch model_preference}}{{#case 'fastest'}}gpt-3.5-turbo{{/case}}{{#case 'balanced'}}gpt-4{{/case}}{{#case 'highest_quality'}}gpt-4{{/case}}{{/switch}}"
        api_key: "{{env.OPENAI_API_KEY}}"
        timeout: 180
        system_prompt: "Process the following batch of {{batch_size}} tasks efficiently."
        user_prompt: "{{#switch prompt_complexity}}{{#case 'simple'}}Summarize: 'The quick brown fox jumps over the lazy dog.' Repeat this {{batch_size}} times with different animals.{{/case}}{{#case 'moderate'}}Generate {{batch_size}} different programming interview questions with solutions.{{/case}}{{#case 'complex'}}Create {{batch_size}} detailed business plans for different startup ideas in the tech industry.{{/case}}{{/switch}}"
    hidden: true

  # ===== WEBSOCKET PERFORMANCE TESTS =====

  - name: "websocket_connection_test"
    description: "Test WebSocket connection establishment and message throughput"
    inputSchema:
      type: "object"
      properties:
        websocket_url:
          type: "string"
          description: "WebSocket server URL"
          default: "wss://echo.websocket.org"
          format: "uri"
        message_count:
          type: "integer"
          description: "Number of messages to send"
          default: 10
          minimum: 1
          maximum: 100
        message_size:
          type: "string"
          description: "Size of each message"
          enum: ["small", "medium", "large"]
          default: "medium"
        connection_timeout:
          type: "integer"
          description: "Connection timeout in seconds"
          default: 30
          minimum: 5
          maximum: 120
      required: []
    routing:
      type: "websocket"
      config:
        url: "{{websocket_url}}"
        headers:
          X-Performance-Test: "connection-throughput"
          X-Message-Count: "{{message_count}}"
          X-Message-Size: "{{message_size}}"
        message: |
          {
            "test_type": "performance_test",
            "message_count": {{message_count}},
            "message_size": "{{message_size}}",
            "timestamp": "{{now}}",
            "payload": "{{#switch message_size}}{{#case 'small'}}{{'x'.repeat(100)}}{{/case}}{{#case 'medium'}}{{'x'.repeat(1000)}}{{/case}}{{#case 'large'}}{{'x'.repeat(10000)}}{{/case}}{{/switch}}"
          }
        timeout: "{{connection_timeout}}"
    hidden: true

  # ===== MIXED WORKLOAD TESTS =====

  - name: "mixed_agent_benchmark"
    description: "Benchmark mixed workload across all agent types"
    inputSchema:
      type: "object"
      properties:
        test_duration:
          type: "integer"
          description: "Total test duration in seconds"
          default: 60
          minimum: 30
          maximum: 300
        agent_distribution:
          type: "object"
          description: "Distribution of agent types"
          properties:
            subprocess_percent:
              type: "integer"
              default: 25
            http_percent:
              type: "integer"
              default: 25
            llm_percent:
              type: "integer"
              default: 25
            websocket_percent:
              type: "integer"
              default: 25
        load_pattern:
          type: "string"
          description: "Load pattern to simulate"
          enum: ["constant", "ramp_up", "spike", "wave"]
          default: "constant"
      required: []
    routing:
      type: "subprocess"
      config:
        command: "bash"
        args: [
          "-c",
          "echo 'Mixed workload benchmark starting'; echo 'Duration: {{test_duration}}s'; echo 'Distribution: subprocess={{agent_distribution.subprocess_percent}}%, http={{agent_distribution.http_percent}}%, llm={{agent_distribution.llm_percent}}%, websocket={{agent_distribution.websocket_percent}}%'; echo 'Load pattern: {{load_pattern}}'; sleep 5; echo 'Benchmark simulation completed'"
        ]
        timeout: "{{test_duration + 30}}"
    hidden: true

  - name: "resource_utilization_monitor"
    description: "Monitor resource utilization during agent execution"
    inputSchema:
      type: "object"
      properties:
        monitoring_duration:
          type: "integer"
          description: "How long to monitor in seconds"
          default: 30
          minimum: 10
          maximum: 300
        sample_interval:
          type: "integer"
          description: "Sampling interval in seconds"
          default: 5
          minimum: 1
          maximum: 30
        metrics_to_collect:
          type: "array"
          description: "Metrics to collect"
          items:
            type: "string"
            enum: ["cpu", "memory", "disk_io", "network", "process_count"]
          default: ["cpu", "memory"]
      required: []
    routing:
      type: "subprocess"
      config:
        command: "bash"
        args: [
          "-c",
          "echo 'Resource monitoring started'; for i in $(seq 1 $(({{monitoring_duration}} / {{sample_interval}}))); do echo '=== Sample $i ==='; {{#each metrics_to_collect}}{{#if @first}}{{else}} && {{/if}}{{#switch this}}{{#case 'cpu'}}top -bn1 | grep 'Cpu(s)' | head -1{{/case}}{{#case 'memory'}}free -h | head -2{{/case}}{{#case 'disk_io'}}iostat -d 1 1 | tail -n +4{{/case}}{{#case 'network'}}cat /proc/net/dev | head -3{{/case}}{{#case 'process_count'}}ps aux | wc -l{{/case}}{{/switch}}{{/each}}; sleep {{sample_interval}}; done; echo 'Monitoring completed'"
        ]
        timeout: "{{monitoring_duration + 60}}"
    hidden: true
